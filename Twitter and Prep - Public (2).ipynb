{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter and Tableau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.    Live Twitter Feed  - code written by Alex Ross - Please copy and paste this onto a server with python 3 (I have tested this on 3.7\n",
    "\n",
    "#### I have implemented this on a Redhat free tier AWS instance.  I also installed postgres on the same instance.  When i ran the python script i used the command (nohup python myscript.py &)  to run it in a background.  Please see Alex Ross's blog for further details (NB: it was originally created for Python 2, i have made subtle changes in the code below to make it work for python 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"https://tableaujunkie.com/post/135404208188/creating-a-live-twitter-feed\">Creating a Live Twitter Feed</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"\"\"<a href=\"https://tableaujunkie.com/post/135404208188/creating-a-live-twitter-feed\">Creating a Live Twitter Feed</a>\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy import Stream\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy.streaming import StreamListener\n",
    "from textblob import TextBlob\n",
    "import json\n",
    "import psycopg2\n",
    "\n",
    "#connect to database\n",
    "try:\n",
    "    conn = psycopg2.connect(\"dbname='<dbname>' user='<username>' host='<host>' password='<password>'\")\n",
    "except:\n",
    "    print (\"I am unable to connect to the database\")\n",
    "\n",
    "#consumer key, consumer secret, access token, access secret.\n",
    "ckey='<customer key>'\n",
    "csecret='<customer secret>'\n",
    "atoken='<access token>'\n",
    "asecret='<access secret>'\n",
    "\n",
    "#lookup arrays to convert user's timezone to a country\n",
    "TimeZones = ['International Date Line West','Midway Island','American Samoa','Hawaii','Alaska','Pacific Time (US & Canada)','Tijuana','Mountain Time (US & Canada)','Arizona','Chihuahua','Mazatlan','Central Time (US & Canada)','Saskatchewan','Guadalajara','Mexico City','Monterrey','Central America','Eastern Time (US & Canada)','Indiana (East)','Bogota','Lima','Quito','Atlantic Time (Canada)','Caracas','La Paz','Santiago','Newfoundland','Brasilia','Buenos Aires','Montevideo','Georgetown','Greenland','Mid-Atlantic','Azores','Cape Verde Is.','Dublin','Edinburgh','Lisbon','London','Casablanca','Monrovia','UTC','Belgrade','Bratislava','Budapest','Ljubljana','Prague','Sarajevo','Skopje','Warsaw','Zagreb','Brussels','Copenhagen','Madrid','Paris','Amsterdam','Berlin','Bern','Rome','Stockholm','Vienna','West Central Africa','Bucharest','Cairo','Helsinki','Kyiv','Riga','Sofia','Tallinn','Vilnius','Athens','Istanbul','Minsk','Jerusalem','Harare','Pretoria','Kaliningrad','Moscow','St. Petersburg','Volgograd','Samara','Kuwait','Riyadh','Nairobi','Baghdad','Tehran','Abu Dhabi','Muscat','Baku','Tbilisi','Yerevan','Kabul','Ekaterinburg','Islamabad','Karachi','Tashkent','Chennai','Kolkata','Mumbai','New Delhi','Kathmandu','Astana','Dhaka','Sri Jayawardenepura','Almaty','Novosibirsk','Rangoon','Bangkok','Hanoi','Jakarta','Krasnoyarsk','Beijing','Chongqing','Hong Kong','Urumqi','Kuala Lumpur','Singapore','Taipei','Perth','Irkutsk','Ulaanbaatar','Seoul','Osaka','Sapporo','Tokyo','Yakutsk','Darwin','Adelaide','Canberra','Melbourne','Sydney','Brisbane','Hobart','Vladivostok','Guam','Port Moresby','Magadan','Srednekolymsk','Solomon Is.','New Caledonia','Fiji','Kamchatka','Marshall Is.','Auckland','Wellington',\"Nuku'alofa\",'Tokelau Is.','Chatham Is.','Samoa','Europe/London','America/New_York','EST']\n",
    "Country = ['United States Minor Outlying Islands','United States Minor Outlying Islands','American Samoa','United States','United States','United States','Mexico','United States','United States','Mexico','Mexico','United States','Canada','Mexico','Mexico','Mexico','Guatemala','United States','United States','Colombia','Peru','Peru','Canada','Venezuela','Bolivia','Chile','Canada','Brazil','Argentina','Uruguay','Guyana','Greenland','South Georgia and the South Sandwich Islands','Portugal','Cape Verde','Ireland','United Kingdom','Portugal','United Kingdom','Morocco','Liberia','#N/A','Serbia','Slovakia','Hungary','Slovenia','Czech Republic','Bosnia and Herzegovina','Macedonia','Poland','Croatia','Belgium','Denmark','Spain','France','Netherlands','Germany','Germany','Italy','Sweden','Austria','Algeria','Romania','Egypt','Finland','Ukraine','Latvia','Bulgaria','Estonia','Lithuania','Greece','Turkey','Belarus','Israel','Zimbabwe','South Africa','Russia','Russia','Russia','Russia','Russia','Kuwait','Saudi Arabia','Kenya','Iraq','Iran','Oman','Oman','Azerbaijan','Georgia','Armenia','Afghanistan','Russia','Pakistan','Pakistan','Uzbekistan','India','India','India','India','Nepal','Bangladesh','Bangladesh','Sri Lanka','Kazakhstan','Russia','Myanmar','Thailand','Thailand','Indonesia','Russia','China','#N/A','Hong Kong','China','Malaysia','Singapore','Taiwan','Australia','Russia','Mongolia','South Korea','Japan','Japan','Japan','Russia','Australia','Australia','Australia','Australia','Australia','Australia','Australia','Russia','Guam','Papua New Guinea','Russia','Russia','Solomon Islands','New Caledonia','Fiji','Russia','Marshall Islands','New Zealand','New Zealand','Tonga','Tokelau','New Zealand','Samoa','United Kingdom','United States','United States']\n",
    "\n",
    "#listner for twitter stream\n",
    "class listener(StreamListener):\n",
    "        def on_data(self, data):\n",
    "           # print(data)\n",
    "            all_data = json.loads(data)\n",
    "\n",
    "            try:\n",
    "                tweet = TextBlob(all_data[\"text\"])\n",
    "                print(tweet.raw)\n",
    "\n",
    "                # determine if sentiment is positive, negative, or neutral\n",
    "                if tweet.sentiment.polarity < 0:\n",
    "                    sentiment = \"negative\"\n",
    "                elif tweet.sentiment.polarity == 0:\n",
    "                    sentiment = \"neutral\"\n",
    "                else:\n",
    "                    sentiment = \"positive\"\n",
    "\n",
    "                created_at = all_data[\"created_at\"]\n",
    "                favorite_count = all_data[\"favorite_count\"]\n",
    "                favorited = all_data[\"favorited\"]\n",
    "                filter_level = all_data[\"filter_level\"]\n",
    "                id_str = all_data[\"id_str\"]\n",
    "                lang = all_data[\"lang\"]\n",
    "                retweet_count = all_data[\"retweet_count\"]\n",
    "                retweeted = all_data[\"retweeted\"]\n",
    "                source = all_data[\"source\"]\n",
    "                timestamp_ms = all_data[\"timestamp_ms\"]\n",
    "                truncated = all_data[\"truncated\"]\n",
    "                user_description = all_data[\"user\"][\"description\"]\n",
    "                user_favourites_count = all_data[\"user\"][\"favourites_count\"]\n",
    "                user_followers_count = all_data[\"user\"][\"followers_count\"]\n",
    "                user_friends_count = all_data[\"user\"][\"friends_count\"]\n",
    "                user_id_str = all_data[\"user\"][\"id_str\"]\n",
    "                user_location = all_data[\"user\"][\"location\"]\n",
    "                user_name = all_data[\"user\"][\"name\"]\n",
    "                user_profile_image_url = all_data[\"user\"][\"profile_image_url\"]\n",
    "                user_screen_name = all_data[\"user\"][\"screen_name\"]\n",
    "                user_statuses_count = all_data[\"user\"][\"statuses_count\"]\n",
    "                user_time_zone = all_data[\"user\"][\"time_zone\"]\n",
    "\n",
    "                #convert hastags array into comma separated string\n",
    "                hashtags = all_data[\"entities\"][\"hashtags\"]\n",
    "                hashtags_str = \"\"\n",
    "                for i in range(0,len(hashtags),1):\n",
    "                    hashtags_str = str(hashtags_str) + hashtags[i][\"text\"]\n",
    "                    if i + 1 < len(hashtags):\n",
    "                        hashtags_str = hashtags_str + \",\"\n",
    "\n",
    "                #convert urls into comma separated strinf\n",
    "                urls = all_data[\"entities\"][\"urls\"]\n",
    "                urls_str = \"\"\n",
    "                for i in range(0,len(urls),1):\n",
    "                    urls_str = str(urls_str) + urls[i][\"url\"]\n",
    "                    if i + 1 < len(urls):\n",
    "                        urls_str = urls_str + \",\"\n",
    "\n",
    "                #convert user timezone into country\n",
    "                user_timezone_country = ''\n",
    "                if user_time_zone in TimeZones:\n",
    "                    index = TimeZones.index(user_time_zone)\n",
    "                    user_timezone_country= Country[index]\n",
    "                c = conn.cursor()\n",
    "\n",
    "                #insert data into database\n",
    "                c.execute(\"INSERT INTO Twitter (tweet,created_at,timestamp_ms,favorite_count,favorited,filter_level,id_str,lang,retweet_count,retweeted,source,truncated,\"\n",
    "                          \"user_description,user_favourites_count,user_followers_count,user_friends_count,user_id_str,user_location,user_name,user_profile_image_url,\"\n",
    "                          \"user_screen_name,user_statuses_count,user_time_zone,hashtags,urls,user_timezone_country,polarity,sentiment) VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\",\n",
    "                    (tweet.raw,created_at,timestamp_ms,favorite_count,favorited,filter_level,id_str,lang,retweet_count,retweeted,source,truncated,user_description,user_favourites_count,\n",
    "                     user_followers_count,user_friends_count,user_id_str,user_location,user_name,user_profile_image_url,user_screen_name,user_statuses_count,user_time_zone,hashtags_str,urls_str,user_timezone_country,tweet.sentiment.polarity,sentiment))\n",
    "\n",
    "                conn.commit()\n",
    "\n",
    "                return(True)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print (all_data)\n",
    "\n",
    "            def on_error(self, status):\n",
    "                print (status)\n",
    "\n",
    "\n",
    "auth = OAuthHandler(ckey, csecret)\n",
    "auth.set_access_token(atoken, asecret)\n",
    "\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        twitterStream = Stream(auth, listener())\n",
    "        twitterStream.filter(track=['@ExpoNHS','#ExpoNHS','#Expo19NHS','@Expo19NHS']) #insert keywords for Twitter search\n",
    "        #twitterStream.sample() #random sample from twitter\n",
    "\n",
    "    except:\n",
    "        # Oh well, reconnect and keep going\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.   Load 7 days of Tweets from the Twitter Rest API - and search based on data loaded in prep.\n",
    "##### Beware - usage limits apply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below is a dataframe to test the function with before using with prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "search = pd.DataFrame([('@ExpoNHS OR #ExpoNHS OR #Expo19NHS OR @Expo19NHS','event',10),('Tableau','Tableau',10)])\n",
    "search.columns=(['keywords','subject','rows'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below is the final code that will work with prep - the dataframe will be the passed from prep.  In this example, the code is looking for a dataframe which has the following columns:  keywords, subject and rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import csv\n",
    "import pandas as pd\n",
    "import json\n",
    "from textblob import TextBlob\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "def twitterfeed(search):\n",
    "\n",
    "    consumer_key=\"<customer_key>\"\n",
    "    consumer_secret=\"<customer_secret>\"\n",
    "    access_token=\"<access_token>\"\n",
    "    access_token_secret=\"<access_token_secret>\"\n",
    "\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    api = tweepy.API(auth)\n",
    "\n",
    "\n",
    "    tweetall = pd.DataFrame()\n",
    "    for subject in search.index:\n",
    "        tweetsdf = pd.DataFrame()\n",
    "    \n",
    "        for tweet in tweepy.Cursor(api.search,q=search.keywords[subject], lang=\"en\",count=10).items(search.rows[subject]):\n",
    "       \n",
    "            tweets=pd.DataFrame()\n",
    "            tweets['created_at'] = [tweet.created_at]\n",
    "            tweets['created_at'] = tweets.created_at.map(lambda x: dt.datetime.strftime(x, '%Y-%m-%dT%H:%M:%SZ'))\n",
    "            tweets['tweettext'] = [tweet.text]\n",
    "            tweets['polarity'] = [TextBlob(tweet.text).sentiment.polarity]\n",
    "            if TextBlob(tweet.text).sentiment.polarity < 0:\n",
    "                tweets['sentiment'] = \"negative\"\n",
    "            elif TextBlob(tweet.text).sentiment.polarity > 0:\n",
    "                tweets['sentiment'] = \"positive\"\n",
    "            else:\n",
    "                tweets['sentiment'] = \"neutral\"\n",
    "            \n",
    "            tweets['source'] = [tweet.source]\n",
    "            tweets['id'] = [tweet.id]\n",
    "            tweets['name']= pd.io.json.json_normalize(tweet.user._json).head(1).name\n",
    "            tweets['screen_name']=pd.io.json.json_normalize(tweet.user._json).head(1).screen_name\n",
    "            tweets['location']=pd.io.json.json_normalize(tweet.user._json).head(1).location\n",
    "            tweets['followers']=pd.io.json.json_normalize(tweet.user._json).head(1).followers_count\n",
    "            tweets['friends'] = pd.io.json.json_normalize(tweet.user._json).head(1).friends_count\n",
    "            tweets['user_description']=pd.io.json.json_normalize(tweet.user._json).head(1).description\n",
    "            try:\n",
    "                tweets['hashtag']= pd.io.json.json_normalize(pd.io.json.json_normalize(tweet.entities).hashtags[0]).head(1).text\n",
    "            except:\n",
    "                tweets['hashtag']=''\n",
    "    \n",
    "            tweetsdf = tweetsdf.append(tweets)\n",
    "        tweetsdf['subject']=search.subject[subject]\n",
    "        tweetall = tweetall.append(tweetsdf)\n",
    "    \n",
    "    tweetall = tweetall.reset_index(drop = True)\n",
    "    return tweetall\n",
    "\n",
    "def get_output_schema():\n",
    "    return pd.DataFrame({\n",
    "            'created_at' : prep_datetime(),\n",
    "            'tweettext' : prep_string(),\n",
    "            'polarity' : prep_decimal(),\n",
    "            'sentiment' : prep_string(),\n",
    "            'source' : prep_string(),\n",
    "            'id' : prep_int(),\n",
    "            'name' : prep_string(),\n",
    "            'screen_name': prep_string(),\n",
    "            'location' : prep_string(),\n",
    "            'followers' : prep_int(),\n",
    "            'friends' : prep_int(),\n",
    "            'user_description': prep_string(),\n",
    "            'hashtag': prep_string(),\n",
    "            'subject' : prep_string()\n",
    "        });"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets look at how this works:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following code connects to twitter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the following libraries are imported\n",
    "\n",
    "import tweepy\n",
    "import csv\n",
    "import pandas as pd\n",
    "import json\n",
    "from textblob import TextBlob\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here, you enter your API key details from twitter.  You need to sign up for a twitter developer account and fill in the form to request approval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "consumer_key=\"<CUSTOMER KEY GOES HERE>\"\n",
    "consumer_secret=\"< CUS SECRET GOES HERE>\"\n",
    "access_token=\"<ACCESS TOKEN GOES HERE>\"\n",
    "access_token_secret=\"<ACCESS TOKEN SECRET GOES HERE>\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, to test that your twitter script will work in prep, you need to simulate the dataframe with the required fields that the script needs.  I have created one called search.  In prep, the column names you will pass need to be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keywords</th>\n",
       "      <th>subject</th>\n",
       "      <th>rows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@ExpoNHS OR #ExpoNHS OR #Expo19NHS OR @Expo19NHS</td>\n",
       "      <td>event</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tableau OR ironviz</td>\n",
       "      <td>Tableau Tweets</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           keywords         subject  rows\n",
       "0  @ExpoNHS OR #ExpoNHS OR #Expo19NHS OR @Expo19NHS           event     2\n",
       "1                                Tableau OR ironviz  Tableau Tweets     2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search = pd.DataFrame([('@ExpoNHS OR #ExpoNHS OR #Expo19NHS OR @Expo19NHS','event',2),('Tableau OR ironviz','Tableau Tweets',2)])\n",
    "search.columns=(['keywords','subject','rows'])\n",
    "\n",
    "search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### this will search twitter accross each subject specified in the previous dataframe.  In this case it will make 2 searches.  It will also only bring back the number of rows as per specified in the dataframe - in this case its 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @NHSEngland: .@HopeGorton discusses body image, mental health and social media in her new blog post: https://t.co/PVcEHnPscA\n",
      "\n",
      "Join her a…\n",
      "RT @Pers_Care: Good to work with @JohnMcArts planning for a creative session at #Expo19NHS.  And love this blog from @ace_national   What c…\n",
      "RT @dfombnr: Our Forest Department Tableau stood 4th best in the #IndependenceDayIndia celebrations held in the parade grounds of Mahabubna…\n",
      "Quick somebody who is better at data visualization than me make a tableau chart analyzing all the Twitter data on T… https://t.co/F2JQZRiYZU\n"
     ]
    }
   ],
   "source": [
    "for subject in search.index:\n",
    "    tweetsdf = pd.DataFrame()\n",
    "\n",
    "\n",
    "    for tweet in tweepy.Cursor(api.search,q=search.keywords[subject], lang=\"en\",count=10).items(search.rows[subject]):\n",
    "        print (tweet.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we have tested the twitter connection, we will now add the data we want to a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetall = pd.DataFrame()\n",
    "for subject in search.index:\n",
    "    tweetsdf = pd.DataFrame()\n",
    "    \n",
    "    for tweet in tweepy.Cursor(api.search,q=search.keywords[subject], lang=\"en\",count=10).items(search.rows[subject]):\n",
    "       \n",
    "        tweets=pd.DataFrame()\n",
    "        tweets['created_at'] = [tweet.created_at]\n",
    "        tweets['created_at'] = tweets.created_at.map(lambda x: dt.datetime.strftime(x, '%Y-%m-%dT%H:%M:%SZ'))\n",
    "        tweets['tweettext'] = [tweet.text]\n",
    "        \n",
    "        #Note: this part uses Text Blob to check for sentement\n",
    "        \n",
    "        tweets['polarity'] = [TextBlob(tweet.text).sentiment.polarity]\n",
    "        if TextBlob(tweet.text).sentiment.polarity < 0:\n",
    "            tweets['sentiment'] = \"negative\"\n",
    "        elif TextBlob(tweet.text).sentiment.polarity > 0:\n",
    "            tweets['sentiment'] = \"positive\"\n",
    "        else:\n",
    "            tweets['sentiment'] = \"neutral\"\n",
    "            \n",
    "        tweets['source'] = [tweet.source]\n",
    "        tweets['id'] = [tweet.id]\n",
    "        \n",
    "        #the info below is contained in nested json.  Json Normalize is a great tool to help with this.\n",
    "        \n",
    "        tweets['name']= pd.io.json.json_normalize(tweet.user._json).head(1).name\n",
    "        tweets['screen_name']=pd.io.json.json_normalize(tweet.user._json).head(1).screen_name\n",
    "        tweets['location']=pd.io.json.json_normalize(tweet.user._json).head(1).location\n",
    "        tweets['followers']=pd.io.json.json_normalize(tweet.user._json).head(1).followers_count\n",
    "        tweets['friends'] = pd.io.json.json_normalize(tweet.user._json).head(1).friends_count\n",
    "        tweets['user_description']=pd.io.json.json_normalize(tweet.user._json).head(1).description\n",
    "        \n",
    "        #Because the availability of a hashtag field is variable - depending on whether one actually exists or not, \n",
    "        #i have added an exception clause of blank.  Also we are only displaying the first hashtag per tweet. If you would like to \n",
    "        #expand that further, you could by using the json normalize function to create an additional hashtag table.  \n",
    "        #However, this will increase the number of rows.\n",
    "        \n",
    "        try:\n",
    "            tweets['hashtag']= pd.io.json.json_normalize(pd.io.json.json_normalize(tweet.entities).hashtags[0]).head(1).text\n",
    "        except:\n",
    "            tweets['hashtag']=''\n",
    "    \n",
    "    tweetsdf = tweetsdf.append(tweets)\n",
    "    tweetsdf['subject']=search.subject[subject]\n",
    "    tweetall = tweetall.append(tweetsdf)\n",
    "    \n",
    "tweetall = tweetall.reset_index(drop = True)\n",
    "\n",
    "tweetall.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create the final function that prep asks for\n",
    "#### To do this i just wrap everything around a function called twitterfeed - i am paramaterising it by search.  That way, it will work with my test dataframe called search, and also with prep as well (as prep will replace search with whatever dataframe it passes through)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twitterfeed(search):\n",
    "\n",
    "    consumer_key=\"<your customer key>\"\n",
    "    consumer_secret=\"<your customer secret>\"\n",
    "    access_token=\"<your access token>\"\n",
    "    access_token_secret=\"<your access token secret>\"\n",
    "\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    api = tweepy.API(auth)\n",
    "\n",
    "\n",
    "    tweetall = pd.DataFrame()\n",
    "    for subject in search.index:\n",
    "        tweetsdf = pd.DataFrame()\n",
    "    \n",
    "        for tweet in tweepy.Cursor(api.search,q=search.keywords[subject], lang=\"en\",count=10).items(search.rows[subject]):\n",
    "       \n",
    "            tweets=pd.DataFrame()\n",
    "            tweets['created_at'] = [tweet.created_at]\n",
    "            tweets['created_at'] = tweets.created_at.map(lambda x: dt.datetime.strftime(x, '%Y-%m-%dT%H:%M:%SZ'))\n",
    "            tweets['tweettext'] = [tweet.text]\n",
    "            tweets['polarity'] = [TextBlob(tweet.text).sentiment.polarity]\n",
    "            if TextBlob(tweet.text).sentiment.polarity < 0:\n",
    "                tweets['sentiment'] = \"negative\"\n",
    "            elif TextBlob(tweet.text).sentiment.polarity > 0:\n",
    "                tweets['sentiment'] = \"positive\"\n",
    "            else:\n",
    "                tweets['sentiment'] = \"neutral\"\n",
    "            \n",
    "            tweets['source'] = [tweet.source]\n",
    "            tweets['id'] = [tweet.id]\n",
    "            tweets['name']= pd.io.json.json_normalize(tweet.user._json).head(1).name\n",
    "            tweets['screen_name']=pd.io.json.json_normalize(tweet.user._json).head(1).screen_name\n",
    "            tweets['location']=pd.io.json.json_normalize(tweet.user._json).head(1).location\n",
    "            tweets['followers']=pd.io.json.json_normalize(tweet.user._json).head(1).followers_count\n",
    "            tweets['friends'] = pd.io.json.json_normalize(tweet.user._json).head(1).friends_count\n",
    "            tweets['user_description']=pd.io.json.json_normalize(tweet.user._json).head(1).description\n",
    "            try:\n",
    "                tweets['hashtag']= pd.io.json.json_normalize(pd.io.json.json_normalize(tweet.entities).hashtags[0]).head(1).text\n",
    "            except:\n",
    "                tweets['hashtag']=''\n",
    "    \n",
    "            tweetsdf = tweetsdf.append(tweets)\n",
    "        tweetsdf['subject']=search.subject[subject]\n",
    "        tweetall = tweetall.append(tweetsdf)\n",
    "    \n",
    "    tweetall = tweetall.reset_index(drop = True)\n",
    "    return tweetall # this is passing the final dataframe back to prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You can test it here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>tweettext</th>\n",
       "      <th>polarity</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>source</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>location</th>\n",
       "      <th>followers</th>\n",
       "      <th>friends</th>\n",
       "      <th>user_description</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2019-08-15T08:05:07Z</td>\n",
       "      <td>The #Expo19NHS theatres give you the chance to...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>positive</td>\n",
       "      <td>Hootsuite Inc.</td>\n",
       "      <td>1161911683020992513</td>\n",
       "      <td>NHS England SW</td>\n",
       "      <td>NHSEnglandSW</td>\n",
       "      <td>South West, England</td>\n",
       "      <td>3766</td>\n",
       "      <td>839</td>\n",
       "      <td>@NHSEngland and @NHSImprovement working togeth...</td>\n",
       "      <td>Expo19NHS</td>\n",
       "      <td>event</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             created_at                                          tweettext  \\\n",
       "6  2019-08-15T08:05:07Z  The #Expo19NHS theatres give you the chance to...   \n",
       "\n",
       "   polarity sentiment          source                   id            name  \\\n",
       "6       0.1  positive  Hootsuite Inc.  1161911683020992513  NHS England SW   \n",
       "\n",
       "    screen_name             location  followers  friends  \\\n",
       "6  NHSEnglandSW  South West, England       3766      839   \n",
       "\n",
       "                                    user_description    hashtag subject  \n",
       "6  @NHSEngland and @NHSImprovement working togeth...  Expo19NHS   event  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitterfeed(search).sample(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally create a prep schema to map the fields returned with fields that prep will expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_schema():\n",
    "    return pd.DataFrame({\n",
    "            'created_at' : prep_datetime(),\n",
    "            'tweettext' : prep_string(),\n",
    "            'polarity' : prep_decimal(),\n",
    "            'sentiment' : prep_string(),\n",
    "            'source' : prep_string(),\n",
    "            'id' : prep_int(),\n",
    "            'name' : prep_string(),\n",
    "            'screen_name': prep_string(),\n",
    "            'location' : prep_string(),\n",
    "            'followers' : prep_int(),\n",
    "            'friends' : prep_int(),\n",
    "            'user_description': prep_string(),\n",
    "            'hashtag': prep_string(),\n",
    "            'subject' : prep_string()\n",
    "        });"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the Tweet into Keywords\n",
    "### I would like to analyse the words used in the tweets - this is how i did it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Below uses sklearn to look at the data taking into account standard english words.  You can change the stop_words to use other dictionaries if you like.  There is plenty of lists to choose from if you search for them.  The dataframe used will be the previously loaded twitter data in prep.  So this script will be placed AFTER ther previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def detlanguage(df):\n",
    "\n",
    "    dataframe = pd.DataFrame()\n",
    "\n",
    "    for A in df.index:\n",
    "        docs = df.head(A+1).tweet.tolist()\n",
    "        cv=CountVectorizer(max_df=1, stop_words='english', max_features=20000)\n",
    "        word_count_vector=cv.fit_transform(docs)\n",
    "        keywords = pd.DataFrame(list(cv.vocabulary_.keys())[:20])\n",
    "        keywords.columns = ['keyword']\n",
    "        keywords['id'] = df.id[A]\n",
    "        keywords['created_at']=df.created_at[A]\n",
    "        dataframe = dataframe.append(keywords)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "\n",
    "def get_output_schema():\n",
    "    return pd.DataFrame({\n",
    "            'keyword' : prep_string(),\n",
    "            'id' : prep_string(),\n",
    "            'created_at' : prep_string()\n",
    "        });\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Below looks at all the tweets as a whole, and excludes the most common words (i.e the words that are used more than 85% of the time).  If you combine this with the previous script in prep and join on keyword as an INNER join, you will only see the words that are deemed important.  I have also implemented sentiment on each word - which won't be as accurate when taken out of context, but still, this will be useful to see overall sentiment of a specific word.\n",
    "\n",
    "I have also tagged each word using Penn Treebank tags \n",
    "https://www.clips.uantwerpen.be/pages/mbsp-tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summarytest\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "import re\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "\n",
    "def overallwithsentiment(df):\n",
    "    print(df.head(1))\n",
    "    #nltk.download('averaged_perceptron_tagger')\n",
    "    #nltk.download('punkt')\n",
    "    def overalllang(df):\n",
    "        docs = df.tweet.tolist()\n",
    "        cv=CountVectorizer(max_df=0.85, stop_words='english', max_features=20000)\n",
    "        word_count_vector=cv.fit_transform(docs)\n",
    "        keywords = pd.DataFrame(list(cv.vocabulary_.keys())[:100])\n",
    "        keywords.columns = ['keyword']\n",
    "        return keywords\n",
    "\n",
    "    #det_language = detlanguage(stop4)\n",
    "    #sum_language = overalllang(stop4)\n",
    "    def print_sentiment_scores(sentence):\n",
    "        snt = analyser.polarity_scores(sentence)\n",
    "        return snt\n",
    "\n",
    "\n",
    "    dataframe = pd.DataFrame()\n",
    "    for A in overalllang(df).index:\n",
    "        score1 = pd.io.json.json_normalize(print_sentiment_scores(overalllang(df).keyword[A]))\n",
    "        score1['keyword'] = overalllang(df).keyword[A]\n",
    "        score1['Wordtag'] = pd.DataFrame(TextBlob(overalllang(df).keyword[A]).tags)[1]\n",
    "        dataframe = dataframe.append(score1)\n",
    "    dataframe = dataframe.reset_index()\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def get_output_schema():\n",
    "    return pd.DataFrame({\n",
    "            'neg' : prep_decimal(),\n",
    "            'neu' : prep_decimal(),\n",
    "            'pos' : prep_decimal(),\n",
    "            'compound' : prep_decimal(),\n",
    "            'keyword' : prep_string(),\n",
    "            'Wordtag' : prep_string()\n",
    "        });"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally, I join the original tweet results back to the keywords so i get all the tweet information as well as the keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
